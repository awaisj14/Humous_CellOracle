#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 16 16:24:35 2023

@author: javed
"""

# Install velocyto from conda using this code if you have M1max MacOS
#conda install -c bioconda velocyto.py

import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scanpy as sc
import scvelo as scvelo
import mygene
sc.settings.verbosity = 3             # verbosity: errors (0), warnings (1), info (2), hints (3)
sc.logging.print_versions()
sc.settings.set_figure_params(dpi=600, facecolor='white')


import pandas as pd
import scanpy as sc

# File paths
counts_path = 'GSE162170_multiome_rna_counts.tsv'
cluster_path = 'GSE162170_multiome_cluster_names.txt'
metadata_path = 'GSE162170_multiome_cell_metadata.txt'

# Function to clean cell IDs (remove leading/trailing whitespaces)
def clean_cell_ids(df, column=None):
    if column:
        df[column] = df[column].astype(str).str.strip()
    else:
        df.index = df.index.astype(str).str.strip()
    return df

# Load RNA counts
counts = pd.read_csv(counts_path, sep='\t', index_col=0)
print("Counts Matrix Shape:", counts.shape)
print(counts.head())

# Load cluster assignments
clusters = pd.read_csv(cluster_path, sep='\t')
print("Cluster Assignments:")
print(clusters.head())

# Identify the column containing cell IDs in clusters
# Replace 'Cell.ID' with the actual column name if different
# For example, if clusters have 'cell_id', adjust accordingly
cell_id_column_clusters = 'Cell.ID'  # Change if necessary

if 'Cell.ID' not in clusters.columns:
    # Example: if the first column is cell IDs
    cell_id_column_clusters = clusters.columns[0]
    clusters.rename(columns={clusters.columns[0]: 'Cell.ID'}, inplace=True)

# Clean cell IDs in clusters
clusters = clean_cell_ids(clusters, column='Cell.ID')

# Set 'Cell.ID' as index for clusters
clusters.set_index('Cell.ID', inplace=True)

# Load cell metadata
metadata = pd.read_csv(metadata_path, sep='\t')
print("Cell Metadata:")
print(metadata.head())

# Clean cell IDs in metadata
metadata = clean_cell_ids(metadata, column='Cell.ID')

# Set 'Cell.ID' as index for metadata
metadata.set_index('Cell.ID', inplace=True)

# Merge clusters with metadata
metadata = metadata.join(clusters, how='left')
print("Merged Metadata:")
print(metadata.head())

# Align counts and metadata
counts_cells = set(counts.columns)
metadata_cells = set(metadata.index)

common_cells = counts_cells.intersection(metadata_cells)
print(f"Number of common cells: {len(common_cells)}")

# Subset counts and metadata to common cells
counts = counts.loc[:, common_cells]
metadata = metadata.loc[common_cells]

# Optional: Reorder columns in counts to match metadata
counts = counts.loc[:, metadata.index]

# Optional: Further clean cell IDs if necessary
# Example: ensure all cell IDs are uppercase/lowercase if case sensitivity is an issue
# counts.columns = counts.columns.str.upper()
# metadata.index = metadata.index.str.upper()

# Create AnnData object
adata = sc.AnnData(X=counts.transpose(), obs=metadata.copy())

# Assign gene names to var
adata.var_names = counts.index.tolist()

# Make gene names unique if necessary
adata.var_names_make_unique()

# Verify uniqueness
assert adata.var_names.is_unique, "Gene names are not unique after making them unique."

print("AnnData object created successfully:")
print(adata)




alladata = adata

adata = alladata

import pandas as pd
import scanpy as sc
import mygene

# Step 1: Load or Initialize the AnnData Object
# Assuming your AnnData object is already loaded and named 'adata'
# If not, load it first. For example:
# adata = sc.read('GSE162170_multiome_adata.h5ad')

# Ensure 'adata' is defined
try:
    adata
except NameError:
    raise ValueError("The AnnData object 'adata' is not defined. Please load your data first.")

# Step 2: Extract Ensembl IDs from adata.var_names
ensembl_ids = adata.var_names.tolist()
print(f"Total Ensembl IDs in adata.var_names: {len(ensembl_ids)}")

# Step 3: Map Ensembl IDs to Gene Symbols Using 'mygene'
mg = mygene.MyGeneInfo()

print("Mapping Ensembl IDs to gene symbols using 'mygene'...")
gene_info = mg.querymany(
    ensembl_ids,
    scopes='ensembl.gene',
    fields='symbol',
    species='human',
    as_dataframe=True
)

# Convert index to column and reset index
gene_df = gene_info.reset_index().rename(columns={'index': 'query'})

print(f"Total mappings retrieved: {len(gene_df)}")

# Step 4: Remove Entries Without Gene Symbols
gene_df = gene_df.dropna(subset=['symbol'])
print(f"Total mappings after removing entries without gene symbols: {len(gene_df)}")

# Step 5: Remove Duplicate Ensembl IDs
ensembl_id_counts = gene_df['query'].value_counts()
duplicate_ensembl_ids = ensembl_id_counts[ensembl_id_counts > 1].index.tolist()
print(f"Number of Ensembl IDs mapping to multiple gene symbols: {len(duplicate_ensembl_ids)}")

if duplicate_ensembl_ids:
    # Remove entries with duplicate Ensembl IDs
    gene_df = gene_df[~gene_df['query'].isin(duplicate_ensembl_ids)]
    print(f"Removed duplicate Ensembl IDs. Remaining mappings: {len(gene_df)}")
else:
    print("No duplicate Ensembl IDs found.")

# Step 6: Remove Gene Symbols That Map to Multiple Ensembl IDs
gene_symbol_counts = gene_df['symbol'].value_counts()
duplicate_gene_symbols = gene_symbol_counts[gene_symbol_counts > 1].index.tolist()
print(f"Number of gene symbols mapping to multiple Ensembl IDs: {len(duplicate_gene_symbols)}")

if duplicate_gene_symbols:
    # Remove entries with duplicate gene symbols
    gene_df = gene_df[~gene_df['symbol'].isin(duplicate_gene_symbols)]
    print(f"Removed duplicate gene symbols. Remaining mappings: {len(gene_df)}")
else:
    print("No duplicate gene symbols found.")

# Step 7: Ensure Unique Mappings Between Ensembl IDs and Gene Symbols
# After steps 5 and 6, gene_df should have unique mappings

# Step 8: Subset 'adata' to Keep Only Genes with Unique Mappings
# Get the set of Ensembl IDs with unique mappings
unique_ensembl_ids = set(gene_df['query'])
print(f"Number of unique Ensembl IDs with unique gene symbols: {len(unique_ensembl_ids)}")

# Find common Ensembl IDs between adata and gene_df
common_ensembl_ids = adata.var_names.intersection(unique_ensembl_ids)
print(f"Number of Ensembl IDs common between adata and gene_df: {len(common_ensembl_ids)}")

# Subset adata to keep only common Ensembl IDs
adata = adata[:, common_ensembl_ids].copy()
print(f"adata now has {adata.n_vars} genes after subsetting.")

# Step 9: Assign Gene Symbols to 'adata.var'
# Set 'query' as index in gene_df for alignment
gene_df.set_index('query', inplace=True)

# Ensure that the adata.var_names are in the same order as gene_df.index
gene_df = gene_df.loc[adata.var_names]

# Assign gene symbols to adata.var
adata.var['gene_symbol'] = gene_df['symbol'].values
print("Assigned gene symbols to adata.var['gene_symbol'].")

# Step 10: Verify Uniqueness of Gene Symbols in 'adata'
if adata.var['gene_symbol'].duplicated().any():
    print("Error: Gene symbols in adata are still duplicated after removing duplicates.")
    # Remove duplicated gene symbols from adata
    duplicated_symbols = adata.var['gene_symbol'][adata.var['gene_symbol'].duplicated()].unique()
    print(f"Duplicated gene symbols: {duplicated_symbols}")
    # Keep only genes with unique gene symbols
    adata = adata[:, ~adata.var['gene_symbol'].duplicated()].copy()
    print(f"adata now has {adata.n_vars} genes after removing duplicated gene symbols.")
else:
    print("All gene symbols in adata are unique.")

# Step 11: Set Gene Symbols as 'var_names' (Optional)
adata.var['ensembl_id'] = adata.var_names
adata.var_names = adata.var['gene_symbol']
print("Set gene symbols as adata.var_names.")

# Step 12: Save the Updated AnnData Object
adata.write('GSE162170_multiome_adata_unique_genes.h5ad')
print("Updated AnnData object saved as 'GSE162170_multiome_adata_unique_genes.h5ad'.")


# Step 2: Load the mapping file
mapping_file = 'GSE162170_multiome_cluster_names.txt'
mapping_df = pd.read_csv(mapping_file, sep='\t')  # Adjust 'sep' if necessary

# Step 3: Create the mapping dictionary
cluster_mapping = pd.Series(mapping_df['Cluster.Name'].values, index=mapping_df['Cluster.ID']).to_dict()

# Step 4: Ensure 'seurat_clusters' is string type if necessary
adata.obs['seurat_clusters'] = adata.obs['seurat_clusters'].astype(str)

# Step 5: Map 'seurat_clusters' to 'celltype'
adata.obs['celltype'] = adata.obs['seurat_clusters'].map(cluster_mapping)

# Step 6: Handle missing mappings
missing = adata.obs['celltype'].isna().sum()
if missing > 0:
    print(f"There are {missing} clusters without a corresponding cell type.")
    adata.obs['celltype'] = adata.obs['celltype'].fillna('Unknown')

# Step 7: Verify and save
print(adata.obs[['seurat_clusters', 'celltype']].head())
adata.write('updated_data_with_celltype.h5ad')


adata = sc.read_h5ad("updated_data_with_celltype.h5ad")

sc.pl.violin(adata, ["RNA.Counts"], groupby="celltype")

# Filter cells with nCount_RNA <= 20000
sc.pp.filter_cells(adata,  max_counts=20000)
sc.pp.filter_cells(adata,  min_counts=1500)

sc.pl.violin(adata, ["RNA.Counts"], groupby="celltype")

# Only consider genes with more than 1 count
sc.pp.filter_genes(adata, min_counts=1)


# Convert to float32 for memory efficiency or float64 if higher precision is needed
adata.X = adata.X.astype('float32')  # or 'float64'
# Normalize gene expression matrix with total UMI count per cell
sc.pp.normalize_per_cell(adata, key_n_counts='RNA.Counts')


# Retrieve unique cell types
unique_celltypes = adata.obs['celltype'].unique()

sc.pl.violin(adata_old, ["DLX6"], groupby="celltype")


# Display the unique cell types
print("Unique Cell Types:")
for celltype in unique_celltypes:
    print(celltype)
# Step 3: Rename 'CluN3' to 'GluN3' using replace()
adata.obs['celltype'] = adata.obs['celltype'].replace({'RG': 'IP'})
adata.obs['celltype'] = adata.obs['celltype'].replace({'GluN6': 'RG'})
adata.obs['celltype'] = adata.obs['celltype'].replace({'CluN3': 'N'})
adata.obs['celltype'] = adata.obs['celltype'].replace({'GluN5': 'N'})
adata.obs['celltype'] = adata.obs['celltype'].replace({'GluN4': 'N'})
adata.obs['celltype'] = adata.obs['celltype'].replace({'GluN2': 'N'})
adata.obs['celltype'] = adata.obs['celltype'].replace({'nIPC/GluN1': 'RG'})
adata.obs['celltype'] = adata.obs['celltype'].replace({'GluN1': 'IN'})

# Define the target clusters to subset
target_clusters = ['N', 'RG', 'IP']

# Create a boolean mask for cells in the target clusters
mask = adata.obs['celltype'].isin(target_clusters)

# Subset the AnnData object
adata_subset = adata[mask].copy()

print(f"Original number of cells: {adata.n_obs}")
print(f"Subsetted number of cells: {adata_subset.n_obs}")
sc.pl.violin(adata_subset, ["SOX2"], groupby="celltype")
sc.pl.violin(adata_subset, ["JUNB"], groupby="celltype")

sc.pl.violin(adata_subset, ["EOMES"], groupby="celltype")

adata = adata_subset


sc.pl.violin(adata, ["JUNB"], groupby="celltype")


# Renormalize after filtering
sc.pp.normalize_per_cell(adata)


# keep raw cont data before log transformation
adata.raw = adata
adata.layers["raw_count"] = adata.raw.X.copy()


# Log transformation and scaling
sc.pp.log1p(adata)
sc.pp.scale(adata)

# PCA
sc.tl.pca(adata, svd_solver='arpack')

# Diffusion map
 # Compute the neighborhood graph if not already done
sc.pp.neighbors(adata, n_neighbors=30, n_pcs=40)
sc.tl.diffmap(adata)
# Calculate neihbors again based on diffusionmap


sc.tl.umap(adata)

sc.pl.violin(adata, ["JUNB"], groupby="celltype")

sc.pl.umap(adata, color="celltype")


# Perform Leiden clustering
sc.tl.leiden(
    adata, 
    resolution=0.75, 
    random_state=42, 
    key_added='leiden'
)

sc.pl.umap(adata, color="leiden")


# Identify marker genes for Leiden clusters using the Wilcoxon rank-sum test
sc.tl.rank_genes_groups(
    adata, 
    groupby='leiden', 
    method='wilcoxon',  # Other options: 't-test', 'logreg'
    corr_method='bonferroni',  # Correction method for multiple testing
    n_genes=25,  # Number of genes to test per group
    key_added='rank_genes_leiden'  # Name for the results in adata.uns
)

# Access the ranking results
rank_genes = adata.uns['rank_genes_leiden']

# Extract names, scores, and p-values
gene_names = rank_genes['names']  # List of gene names per cluster
scores = rank_genes['scores']      # Scores (e.g., log fold-change) per gene
pvals = rank_genes['pvals']        # P-values per gene
pvals_adj = rank_genes['pvals_adj']  # Adjusted p-values per gene

import pandas as pd

# Number of top genes to extract
top_n = 20  # Change as needed

# Get the list of clusters
clusters = rank_genes['names'].dtype.names

# Initialize a dictionary to store top genes per cluster
top_genes_dict = {}

for cluster in clusters:
    top_genes_dict[cluster] = gene_names[cluster][:top_n]
    
# Convert the dictionary to a DataFrame
top_genes_df = pd.DataFrame(top_genes_dict)
print(top_genes_df)

sc.pl.umap(adata, color="JUNB")
sc.pl.umap(adata, color="SOX2")
sc.pl.umap(adata, color="EOMES")
sc.pl.umap(adata, color="BCL11B")


leiden_mapping = {
    '0': 'N',
    '1': 'Diff_N',
    '2': 'N',
    '3': 'IP',
    '4': 'RG',
    '5': 'N',
    '6': 'Diff_N',
    '7': 'N',
    '8': 'N',
    '9': 'IP',
    '10': 'Oligo',
    '11': 'IN'
}

# Create a new column with the renamed clusters
adata.obs['celltype'] = adata.obs['leiden'].map(leiden_mapping)

target_clusters = ['N', 'RG', 'IP']

# Create a boolean mask for cells in the target clusters
mask = adata.obs['celltype'].isin(target_clusters)

# Subset the AnnData object
adata_subset = adata[mask].copy()

adata = adata_subset

sc.pp.neighbors(adata, n_neighbors=30, n_pcs=40)
sc.tl.diffmap(adata)
# Calculate neihbors again based on diffusionmap


sc.tl.umap(adata)

sc.pl.umap(adata, color="celltype")


# PAGA graph construction
sc.tl.paga(adata, groups='celltype')
plt.rcParams["figure.figsize"] = [6, 4.5]
sc.pl.paga(adata)

sc.tl.draw_graph(adata, init_pos='paga', random_state=123)
sc.pl.draw_graph(adata, color='SOX2')
sc.pl.draw_graph(adata, color='EOMES')
sc.pl.draw_graph(adata, color='JUNB')
sc.pl.draw_graph(adata, color='RBFOX3')

sc.pl.draw_graph(adata, color='leiden')
sc.pl.draw_graph(adata, color='celltype')


G1S_genes_Tirosh = ['MCM5', 'PCNA', 'TYMS', 'FEN1', 'MCM2', 'MCM4', 'RRM1', 'UNG', 'GINS2', 'MCM6', 'CDCA7', 'DTL', 'PRIM1', 'UHRF1', 'MLF1IP', 'HELLS', 'RFC2', 'RPA2', 'NASP', 'RAD51AP1', 'GMNN', 'WDR76', 'SLBP', 'CCNE2', 'UBR7', 'POLD3', 'MSH2', 'ATAD2', 'RAD51', 'RRM2', 'CDC45', 'CDC6', 'EXO1', 'TIPIN', 'DSCC1', 'BLM', 'CASP8AP2', 'USP1', 'CLSPN', 'POLA1', 'CHAF1B', 'BRIP1', 'E2F8']
G2M_genes_Tirosh = ['HMGB2', 'CDK1', 'NUSAP1', 'UBE2C', 'BIRC5', 'TPX2', 'TOP2A', 'NDC80', 'CKS2', 'NUF2', 'CKS1B', 'MKI67', 'TMPO', 'CENPF', 'TACC3', 'FAM64A', 'SMC4', 'CCNB2', 'CKAP2L', 'CKAP2', 'AURKB', 'BUB1', 'KIF11', 'ANP32E', 'TUBB4B', 'GTSE1', 'KIF20B', 'HJURP', 'CDCA3', 'HN1', 'CDC20', 'TTK', 'CDC25C', 'KIF2C', 'RANGAP1', 'NCAPD2', 'DLGAP5', 'CDCA2', 'CDCA8', 'ECT2', 'KIF23', 'HMMR', 'AURKA', 'PSRC1', 'ANLN', 'LBR', 'CKAP5', 'CENPE', 'CTCF', 'NEK2', 'G2E3', 'GAS2L3', 'CBX5', 'CENPA']

cell_cycle_genes =     G1S_genes_Tirosh + G2M_genes_Tirosh
print(len(cell_cycle_genes))

s_genes = G1S_genes_Tirosh
g2m_genes = G2M_genes_Tirosh
cell_cycle_genes = [x for x in cell_cycle_genes if x in adata.var_names]
print(len(cell_cycle_genes))
print( set(G1S_genes_Tirosh + G2M_genes_Tirosh) - set(cell_cycle_genes) )

print( set(G1S_genes_Tirosh + G2M_genes_Tirosh) &  set(adata.var.index) )

sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes)
import seaborn as sns

phase_threshold = 0
v1 = adata.obs[ 'S_score']
v2 = adata.obs[ 'G2M_score' ]
str_data_inf = ' Human Multiome '
n_x_subplots = 1
fig = plt.figure(figsize = (20,10) ); c = 0
plt.suptitle(str_data_inf +' '+ str(adata.shape) , fontsize = 20  )

c += 1; fig.add_subplot(1,n_x_subplots ,c)
plt.title('Scanpy phase marks', fontsize = 20 )
v_color =  adata.obs[ 'phase']
ax = sns.scatterplot(x=v1,y=v2, hue = v_color )
# Changing fontsize for the legend: 
plt.setp(ax.get_legend().get_texts(), fontsize=20) # for legend text
plt.setp(ax.get_legend().get_title(), fontsize=20) # for legend title    
plt.xlabel('Scanpy G1S' , fontsize = 20)
plt.ylabel('Scanpy G2M', fontsize = 20 )


plt.show()

threshold = 1.0

# Ensure 'SOX2' and 'VIM' are in adata_old.var_names
if "SOX2" not in adata.var_names or "VIM" not in adata.var_names:
    raise ValueError("Both 'SOX2' and 'VIM' must be present in adata_old.var_names.")

# Extract the expression values for SOX2 and VIM.
# If needed, replace 'X' with the appropriate layer or consider using normalized values from adata_old.layers.
sox2_expr = adata[:, "SOX2"].X
vim_expr = adata[:, "VIM"].X

# Since X can be a sparse matrix, it's safer to convert to a dense array if needed:
# (If your data is large, consider working with sparse logic or summarize differently)
sox2_expr = sox2_expr.toarray().flatten() if hasattr(sox2_expr, 'toarray') else sox2_expr
vim_expr = vim_expr.toarray().flatten() if hasattr(vim_expr, 'toarray') else vim_expr

# Identify RG cells: those that express both SOX2 and VIM above the threshold
rg_cells = (sox2_expr > threshold) & (vim_expr > threshold)

# Update the celltype for these cells. Cells not meeting the criteria remain as is.
adata.obs.loc[rg_cells, 'celltype'] = "RG"

adata.obs['celltype'] = adata.obs['celltype'].astype(str)



adata_old = sc.read_h5ad("updated_data_with_celltype.h5ad")

# Step 1: Copy over 'celltype' from adata_new to adata_old, aligning on cell barcodes
# Extract 'celltype' from adata_new
celltype_series = adata.obs['celltype']

# Align the series to the index of adata_old
celltype_series_aligned = celltype_series.reindex(adata_old.obs.index)

# Assign the aligned 'celltype' to adata_old
adata_old.obs['celltype'] = celltype_series_aligned

# Step 2: Remove cells from adata_old where 'celltype' is NaN
# This filters out cells without 'celltype' annotations
adata_old = adata_old[adata_old.obs['celltype'].notna()].copy()

celltype_series = adata.obs['phase']

# Align the series to the index of adata_old
celltype_series_aligned = celltype_series.reindex(adata_old.obs.index)

# Assign the aligned 'celltype' to adata_old
adata_old.obs['phase'] = celltype_series_aligned

# Step 2: Remove cells from adata_old where 'celltype' is NaN
# This filters out cells without 'celltype' annotations
adata_old = adata_old[adata_old.obs['phase'].notna()].copy()

celltype_series = adata.obs['S_score']

# Align the series to the index of adata_old
celltype_series_aligned = celltype_series.reindex(adata_old.obs.index)

# Assign the aligned 'celltype' to adata_old
adata_old.obs['S_score'] = celltype_series_aligned

# Step 2: Remove cells from adata_old where 'celltype' is NaN
# This filters out cells without 'celltype' annotations
adata_old = adata_old[adata_old.obs['S_score'].notna()].copy()

celltype_series = adata.obs['G2M_score']

# Align the series to the index of adata_old
celltype_series_aligned = celltype_series.reindex(adata_old.obs.index)

# Assign the aligned 'celltype' to adata_old
adata_old.obs['G2M_score'] = celltype_series_aligned

# Step 2: Remove cells from adata_old where 'celltype' is NaN
# This filters out cells without 'celltype' annotations
adata_old = adata_old[adata_old.obs['G2M_score'].notna()].copy()


adata_old.write('updated_data_with_celltype_phase_redone.h5ad')

#### NOW RUN THE R CODE TO RENAME THE VIM+SOX2+ CELLS AS RGS


adata = sc.read_h5ad("updated_data_with_celltype_phase_redone.h5ad")

sc.pp.filter_genes(adata, min_counts=1)


# Convert to float32 for memory efficiency or float64 if higher precision is needed
adata.X = adata.X.astype('float32')  # or 'float64'
# Normalize gene expression matrix with total UMI count per cell
sc.pp.normalize_per_cell(adata, key_n_counts='RNA.Counts')


# Select top 2000 highly-variable genes
filter_result = sc.pp.filter_genes_dispersion(adata.X,
                                              flavor='cell_ranger',
                                              n_top_genes=3000,
                                              log=False)

# Subset the genes
adata = adata[:, filter_result.gene_subset]

sc.pl.violin(adata, ["JUNB"], groupby="celltype")


# Renormalize after filtering
sc.pp.normalize_per_cell(adata)


# keep raw cont data before log transformation
adata.raw = adata
adata.layers["raw_count"] = adata.raw.X.copy()


# Log transformation and scaling
sc.pp.log1p(adata)
sc.pp.scale(adata)

# PCA
sc.tl.pca(adata, svd_solver='arpack')

# Diffusion map
 # Compute the neighborhood graph if not already done
sc.pp.neighbors(adata, n_neighbors=7, n_pcs=7)
sc.tl.diffmap(adata)
# Calculate neihbors again based on diffusionmap


sc.tl.umap(adata)

sc.pl.violin(adata, ["JUNB"], groupby="celltype")

sc.pl.umap(adata, color="celltype")
sc.pl.umap(adata, color="phase")
sc.pl.violin(adata, ["JUNB"], groupby="celltype")

# PAGA graph construction
sc.tl.paga(adata, groups='celltype')
plt.rcParams["figure.figsize"] = [6, 4.5]
sc.pl.paga(adata)

sc.tl.draw_graph(adata, init_pos='X_umap', random_state=123)
sc.pl.draw_graph(adata, color='JUNB')
sc.pl.draw_graph(adata, color='celltype')


adata.write_h5ad("Trevino_Final_reannotated_phase_celloracle_3000_redone_v3.h5ad")
adata = sc.read_h5ad("Trevino_Final_reannotated_phase_celloracle_3000_redone_v3.h5ad")


# 1. Extract gene names
genes = adata.var_names.tolist()

# 2. Write to a text file
output_path = "genes_list.txt"
with open(output_path, "w") as txtfile:
    for g in genes:
        txtfile.write(g + "\n")

print(f"Wrote {len(genes)} genes to {output_path}")



import copy
import glob
import time
import os
import shutil
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scanpy as sc
import seaborn as sns
from tqdm.auto import tqdm



import os, sys, shutil, importlib, glob
from tqdm.notebook import tqdm

#install bedtools2 in your conda environment
#git clone https://github.com/arq5x/bedtools2.git
#cd bedtools2
#make clean; make all
#bin/bedtools --version
#bedtools v2.20.1-4-gb877b35


#import time
import velocyto
import celloracle as co
from celloracle.applications import Pseudotime_calculator
co.__version__

from celloracle import motif_analysis as ma
import celloracle as co
co.__version__

from pybedtools import BedTool



#plt.rcParams["font.family"] = "arial"
plt.rcParams["figure.figsize"] = [5,5]
%config InlineBackend.figure_format = 'retina'
plt.rcParams["savefig.dpi"] = 300

%matplotlib inline




pt = Pseudotime_calculator(adata=adata,
                           obsm_key="X_draw_graph_fa", # Dimensional reduction data name
                           cluster_column_name="celltype" # Clustering data name
                           )


print("Clustering name: ", pt.cluster_column_name)
print("Cluster list", pt.cluster_list)
# Check data
pt.plot_cluster(fontsize=8)



# Here, clusters can be classified into either MEP lineage or GMP lineage

clusters_in_Diff_lineage = ['RG','IP','N']

# Make a dictionary
lineage_dictionary = {"Lineage_Diff": clusters_in_Diff_lineage}

# Input lineage information into pseudotime object
pt.set_lineage(lineage_dictionary=lineage_dictionary)

# Visualize lineage information
pt.plot_lineages()


import plotly.express as px
import plotly.io as pio
pio.renderers.default = "browser"
try:
    import plotly.express as px
    def plot(adata, embedding_key, cluster_column_name):
        embedding = adata.obsm[embedding_key]
        df = pd.DataFrame(embedding, columns=["x", "y"])
        df["cluster"] = adata.obs[cluster_column_name].values
        df["label"] = adata.obs.index.values
        fig = px.scatter(df, x="x", y="y", hover_name=df["label"], color="cluster")
        fig.show()

    plot(adata=pt.adata,
         embedding_key=pt.obsm_key,
         cluster_column_name=pt.cluster_column_name)
except:
    print("Plotly not found in your environment. Did you install plotly? Please read the instruction above.")

# Estimated root cell name for each lineage
root_cells = {"Lineage_Diff": "hft_ctx_w21_dc2r2_r1_CCCTCAGTCTTGAGCG"}
pt.set_root_cells(root_cells=root_cells)


pt.plot_root_cells()

"umap" in pt.adata.obsm

pt.get_pseudotime_per_each_lineage()

# Check results
pt.plot_pseudotime(cmap="rainbow")


pt.adata.obs[["Pseudotime"]].head()


adata.obs = pt.adata.obs

# Save updated anndata object
adata.write_h5ad("Final_celloracle_v2.h5ad")

adata = sc.read_h5ad("Final_celloracle_v2.h5ad")


# export the celltype annotation and cell names as csv for R, which can used for annotated the ATAC counts
df = adata.obs[['celltype']].copy()

# 3. Move the index (which holds cell barcodes) into a column
df.reset_index(inplace=True)  # Now df has a column "index" containing the cell names

# 4. Rename the columns to "cell" and "celltype" for clarity
df.columns = ['cell', 'celltype']

# 5. Write to CSV
df.to_csv("/Users/javed/Documents/Human_multiome/Mouse_human_promoter_accessibility/celltypes.csv", index=False)


print(f"Cell number is :{adata.shape[0]}")
print(f"Gene number is :{adata.shape[1]}")

# Random downsampling into 30K cells if the anndata object include more than 30 K cells.
n_cells_downsample = 30000
if adata.shape[0] > n_cells_downsample:
    # Let's dowmsample into 30K cells
    sc.pp.subsample(adata, n_obs=n_cells_downsample, random_state=123)

# Define the input and output file paths
input_file = "GSE162170_multiome_atac_consensus_peaks.txt"
output_bed_file = "GSE162170_multiome_atac_consensus_peaks.bed"

# Read the original TXT file
df = pd.read_csv(input_file, sep='\t')

# Select the necessary columns for BED format
# Typically: chrom, start, end, name, score, strand
bed_df = df[['seqnames', 'start', 'end', 'name', 'score', 'strand']]

# Rename 'seqnames' to 'chrom' to match BED format
bed_df = bed_df.rename(columns={'seqnames': 'chrom'})

# Save the DataFrame as a BED file without headers and index
bed_df.to_csv(output_bed_file, sep='\t', header=False, index=False)

print(f"BED file saved to {output_bed_file}")

# Load bed_file
file_path_of_bed_file = "GSE162170_multiome_atac_consensus_peaks.bed"
bed = ma.read_bed(file_path_of_bed_file)
print(bed.shape)
bed.head()

# Convert bed file into peak name list
peaks = ma.process_bed_file.df_to_list_peakstr(bed)
peaks

tss_annotated = ma.get_tss_info(peak_str_list=peaks, ref_genome="hg38")

# Check results
tss_annotated.tail()


# Change format
peak_id_tss = ma.process_bed_file.df_to_list_peakstr(tss_annotated)
tss_annotated = pd.DataFrame({"peak_id": peak_id_tss,
                              "gene_short_name": tss_annotated.gene_short_name.values})
tss_annotated = tss_annotated.reset_index(drop=True)
print(tss_annotated.shape)
tss_annotated.head()


tss_annotated.to_csv("processed_peak_file.csv")


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


import seaborn as sns

import os, sys, shutil, importlib, glob
from tqdm.notebook import tqdm
import celloracle as co
from celloracle import motif_analysis as ma
from celloracle.utility import save_as_pickled_object
co.__version__
'0.13.1'
%config InlineBackend.figure_format = 'retina'
%matplotlib inline

plt.rcParams['figure.figsize'] = (15,7)
plt.rcParams["savefig.dpi"] = 600


ref_genome = "hg38"

genome_installation = ma.is_genome_installed(ref_genome=ref_genome)
print(ref_genome, "installation: ", genome_installation)
if not genome_installation:
    import genomepy
    genomepy.install_genome(name=ref_genome, provider="UCSC")
else:
    print(ref_genome, "is installed.")
    

    
# Load annotated peak data.
peaks = pd.read_csv("processed_peak_file.csv", index_col=0)
peaks.head()

def decompose_chrstr(peak_str):
    """
    Args:
        peak_str (str): peak_str. e.g. 'chr1_3094484_3095479'

    Returns:
        tuple: chromosome name, start position, end position
    """

    *chr_, start, end = peak_str.split("_")
    chr_ = "_".join(chr_)
    return chr_, start, end

from genomepy import Genome

def check_peak_format(peaks_df, ref_genome):
    """
    Check peak format.
     (1) Check chromosome name.
     (2) Check peak size (length) and remove sort DNA sequences (<5bp)

    """

    df = peaks_df.copy()

    n_peaks_before = df.shape[0]

    # Decompose peaks and make df
    decomposed = [decompose_chrstr(peak_str) for peak_str in df["peak_id"]]
    df_decomposed = pd.DataFrame(np.array(decomposed), index=peaks_df.index)
    df_decomposed.columns = ["chr", "start", "end"]
    df_decomposed["start"] = df_decomposed["start"].astype(int)
    df_decomposed["end"] = df_decomposed["end"].astype(int)

    # Load genome data
    genome_data = Genome(ref_genome)
    all_chr_list = list(genome_data.keys())


    # DNA length check
    lengths = np.abs(df_decomposed["end"] - df_decomposed["start"])


    # Filter peaks with invalid chromosome name
    n_threshold = 5
    df = df[(lengths >= n_threshold) & df_decomposed.chr.isin(all_chr_list)]

    # DNA length check
    lengths = np.abs(df_decomposed["end"] - df_decomposed["start"])

    # Data counting
    n_invalid_length = len(lengths[lengths < n_threshold])
    n_peaks_invalid_chr = n_peaks_before - df_decomposed.chr.isin(all_chr_list).sum()
    n_peaks_after = df.shape[0]


    #
    print("Peaks before filtering: ", n_peaks_before)
    print("Peaks with invalid chr_name: ", n_peaks_invalid_chr)
    print("Peaks with invalid length: ", n_invalid_length)
    print("Peaks after filtering: ", n_peaks_after)

    return df


peaks = check_peak_format(peaks, ref_genome)

# Instantiate TFinfo object
tfi = ma.TFinfo(peak_data_frame=peaks,
                ref_genome=ref_genome)

# Scan motifs. !!CAUTION!! This step may take several hours if you have many peaks!
tfi.scan(fpr=0.02,
         motifs=None,  # If you enter None, default motifs will be loaded.
         verbose=True)

# Save tfinfo object
tfi.to_hdf5(file_path="test1.celloracle.tfinfo")
tfi = co.load_hdf5("test1.celloracle.tfinfo")
# Check motif scan results
tfi.scanned_df.head()
tfi.scanned_df.to_csv("base_GRN_dataframe_filtered_scanned.csv")

# Reset filtering
tfi.reset_filtering()

# Do filtering
tfi.filter_motifs_by_score(threshold=10)

# Format post-filtering results.
tfi.make_TFinfo_dataframe_and_dictionary(verbose=True)


df = tfi.to_dataframe()
df.head()


# Save result as a dataframe
df = tfi.to_dataframe()
df.to_parquet("base_GRN_dataframe_filtered.parquet")

df = pd.read_parquet("base_GRN_dataframe_filtered.parquet")   
df.to_csv("base_GRN_dataframe_filtered.csv")




